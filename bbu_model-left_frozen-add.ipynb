{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1bd7532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/sukaraca/opt/anaconda3/lib/python3.9/site-packages (4.19.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/sukaraca/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sukaraca/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: filelock in /Users/sukaraca/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sukaraca/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/sukaraca/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.6.0)\n",
      "Requirement already satisfied: requests in /Users/sukaraca/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/sukaraca/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/sukaraca/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/sukaraca/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sukaraca/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/sukaraca/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sukaraca/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/sukaraca/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sukaraca/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sukaraca/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "environ({'TERM_PROGRAM': 'Apple_Terminal', 'SHELL': '/bin/zsh', 'TERM': 'xterm-color', 'TMPDIR': '/var/folders/wj/wzcmzk2n2232h1jj_2tfgl700000gn/T/', 'CONDA_SHLVL': '1', 'CONDA_PROMPT_MODIFIER': '(base) ', 'TERM_PROGRAM_VERSION': '444', 'TERM_SESSION_ID': '3AF89B3F-A6F1-40E8-A15B-33FC1A27A66A', 'LC_ALL': 'en_US.UTF-8', 'USER': 'sukaraca', 'CONDA_EXE': '/Users/sukaraca/opt/anaconda3/bin/conda', 'SSH_AUTH_SOCK': '/private/tmp/com.apple.launchd.34nraYaV8T/Listeners', '_CE_CONDA': '', 'CONDA_ROOT': '/Users/sukaraca/opt/anaconda3', 'PATH': '/Users/sukaraca/opt/anaconda3/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/Apple/usr/bin', '__CFBundleIdentifier': 'com.apple.Terminal', 'CONDA_PREFIX': '/Users/sukaraca/opt/anaconda3', 'PWD': '/Users/sukaraca', 'LANG': 'en_US.UTF-8', 'EVENT_NOKQUEUE': '1', 'XPC_FLAGS': '0x0', 'XPC_SERVICE_NAME': '0', '_CE_M': '', 'HOME': '/Users/sukaraca', 'SHLVL': '4', 'LOGNAME': 'sukaraca', 'CONDA_PYTHON_EXE': '/Users/sukaraca/opt/anaconda3/bin/python', 'LC_CTYPE': 'UTF-8', 'CONDA_DEFAULT_ENV': 'base', '_': '/Users/sukaraca/opt/anaconda3/bin/jupyter-notebook', '__CF_USER_TEXT_ENCODING': '0x1F5:0x0:0x0', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'JPY_PARENT_PID': '71583', 'CLICOLOR': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://matplotlib_inline.backend_inline', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE'})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "!pip install transformers\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b1c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('polusa_data/left_train.csv')\n",
    "df_test = pd.read_csv('polusa_data/left_test.csv')\n",
    "df_val = pd.read_csv('polusa_data/left_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b69981cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#load model and tokenizer\n",
    "\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased', num_labels=2, return_dict=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445fbc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sukaraca/opt/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenize and encode sequences\n",
    "max_length = 512\n",
    "\n",
    "tokens_train = tokenizer.__call__(\n",
    "    df_train['body'].tolist(),\n",
    "    padding = True,\n",
    "    truncation = True)\n",
    "  \n",
    "tokens_val = tokenizer.__call__(\n",
    "    df_val['body'].tolist(),\n",
    "    padding = True,\n",
    "    truncation = True)\n",
    "  \n",
    "tokens_test = tokenizer.__call__(\n",
    "    df_test['body'].tolist(),\n",
    "    padding = True,\n",
    "    truncation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66419e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the integer sequences to tensors.\n",
    "  \n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(df_train['label'].tolist())\n",
    "  \n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(df_val['label'].tolist())\n",
    "  \n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(df_test['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25918923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f79113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123e91b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THINGS TO BE CAREFUL ABOUT\n",
    "    #FREEZING ONLY 3 LAYERS\n",
    "    #1 DENSE LAYER , 1 OUTPUT LAYER\n",
    "\n",
    "    #defining new layers\n",
    "class BERT_left_layer(nn.Module):\n",
    "  \n",
    "    def __init__(self, bert):\n",
    "        \n",
    "        super(BERT_left_layer, self).__init__()\n",
    "  \n",
    "        self.bert = bert \n",
    "        \n",
    "      # dropout layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "      # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "  \n",
    "      # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        \n",
    "      # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "  \n",
    "      #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "  \n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "  \n",
    "      #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "        \n",
    "        x = self.fc1(cls_hs)\n",
    "  \n",
    "        x = self.relu(x)\n",
    "  \n",
    "        x = self.dropout(x)\n",
    "  \n",
    "      # output layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "    # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7122a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_left_layer(bert)\n",
    "\n",
    "#push the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(),lr = 1e-5)  # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de3af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the class weights\n",
    "class_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(df_train['label']), y = df_train['label'])\n",
    "\n",
    "print(\"Class Weights:\",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e03b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# push to GPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e696ae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine-tuning\n",
    "\n",
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "  \n",
    "  # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    " \n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "    # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "    # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "    # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "    # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "    # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "  # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "\n",
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "    print(\"\\nEvaluating...\")\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "  # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "    # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "      \n",
    "      # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "      # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "      \n",
    "      # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "      # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb8bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine-tuning\n",
    "\n",
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5faebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "\n",
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5d77ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model's performance\n",
    "\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60e5fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "pd.crosstab(test_y, preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
